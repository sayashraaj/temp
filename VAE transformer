import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Define Transformer-based VAE with Beta-VAE Class
class TransformerVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, n_heads=4, num_transformer_layers=2, beta=1.0):
        super(TransformerVAE, self).__init__()
        self.beta = beta  # Beta for Beta-VAE

        # Encoder layers
        self.fc_encode = nn.Linear(input_dim, 128)
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=128, nhead=n_heads), num_layers=num_transformer_layers
        )
        self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_logvar = nn.Linear(128, latent_dim)

        # Decoder layers
        self.fc_decode = nn.Linear(latent_dim, 128)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=128, nhead=n_heads), num_layers=num_transformer_layers
        )
        self.fc_out = nn.Linear(128, input_dim)

        # Apply Xavier initialization
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """Apply Xavier initialization to linear layers."""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                module.bias.data.fill_(0.0)

    def encode(self, x):
        h = F.relu(self.fc_encode(x))
        h = h.unsqueeze(1)  # Add sequence dimension for Transformer
        h = self.transformer_encoder(h)
        h = h.mean(dim=1)  # Global average pooling
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc_decode(z))
        h = h.unsqueeze(1)  # Add sequence dimension for Transformer
        h = self.transformer_decoder(h, h)  # Decode using the same output for self-attention
        return self.fc_out(h.squeeze(1))  # Remove sequence dimension

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# Training Function with Beta-VAE Loss
def train_vae(vae, data_loader, epochs, optimizer):
    recon_losses = []
    kl_losses = []
    
    for epoch in range(epochs):
        epoch_recon_loss = 0
        epoch_kl_loss = 0
        
        for data in data_loader:
            optimizer.zero_grad()
            recon_batch, mu, logvar = vae(data)
            recon_loss = F.mse_loss(recon_batch, data, reduction='sum')  # Reconstruction loss
            
            # KL divergence loss for Beta-VAE (scaled by beta)
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL divergence
            loss = recon_loss + vae.beta * kl_loss  # Apply beta scaling
            
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)

            optimizer.step()
            
            epoch_recon_loss += recon_loss.item()
            epoch_kl_loss += kl_loss.item()

        avg_recon_loss = epoch_recon_loss / len(data_loader.dataset)
        avg_kl_loss = epoch_kl_loss / len(data_loader.dataset)

        recon_losses.append(avg_recon_loss)
        kl_losses.append(avg_kl_loss)
        
        print(f'Epoch {epoch}: Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}')
    
    return recon_losses, kl_losses

# Generate Correlated Data with Different Means and Standard Deviations
def generate_correlated_data(data_size):
    # Mean and standard deviation for each variable
    means = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
    std_devs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    
    # Covariance matrix for 10 correlated features
    cov_matrix = np.array([
        [std_devs[i] * std_devs[j] * 0.8 if i != j else std_devs[i]**2 for j in range(10)] 
        for i in range(10)
    ])

    # Generate correlated data
    data = np.random.multivariate_normal(means, cov_matrix, size=data_size)
    return data

# Generate Data
data_size = 100000
complex_data = generate_correlated_data(data_size)
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(complex_data)
data_tensor = torch.FloatTensor(data_scaled)

# VAE Training
latent_dim = 3  # Latent dimension is reduced to 3
input_dim = 10  # Since we have ten features after scaling

vae = TransformerVAE(input_dim, latent_dim, beta=4.0)  # Beta = 4.0 for Beta-VAE
optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)

# Create DataLoader for batching
data_loader = torch.utils.data.DataLoader(data_tensor, batch_size=128, shuffle=True)

# Train the VAE
recon_losses, kl_losses = train_vae(vae, data_loader, epochs=100, optimizer=optimizer)

#####################
# Inverse Transform the Output
with torch.no_grad():
    # Encode the original data to get the latent representation
    mu, _ = vae.encode(data_tensor[:100])
    reconstructed_data = vae.decode(mu).numpy()  # Use mu as input for decoding
    reconstructed_data = scaler.inverse_transform(reconstructed_data)

# Plotting Losses
plt.figure(figsize=(12, 6))
plt.plot(recon_losses, label='Reconstruction Loss')
plt.plot(kl_losses, label='KL Loss')
plt.title('Losses over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Visualization of Original vs. Reconstructed Data using Histograms
plt.figure(figsize=(12, 6))
plt.hist(complex_data.flatten(), bins=30, alpha=0.5, label='Original Data', density=True)
plt.hist(reconstructed_data.flatten(), bins=30, alpha=0.5, label='Reconstructed Data', density=True)
plt.title('Original vs. Reconstructed Data Distribution')
plt.xlabel('Values')
plt.ylabel('Density')
plt.legend()
plt.show()

# Check Correlation Matrices
original_corr = np.corrcoef(complex_data.T)
reconstructed_corr = np.corrcoef(reconstructed_data.T)

print("Original Data Correlation Matrix:")
print(original_corr)

print("\nReconstructed Data Correlation Matrix:")
print(reconstructed_corr)
