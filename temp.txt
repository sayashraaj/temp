from transformers import BertConfig, BertForMaskedLM
import torch
import pickle
from torch.utils.data import DataLoader, TensorDataset

# Define variables
max_word_length = 15
hidden_size = 256
num_attention_heads = 4
num_hidden_layers = 4
model_path = 'path/to/save/model'
tokenizer_path = 'path/to/save/tokenizer.pkl'
epochs = 3
learning_rate = 1e-4
max_len = 15
batch_size = 32
dictionary = ["apple", "banana", "orange", "grape", "melon", "supercalifragilisticexpialidocious"]

# Define the character-level tokenizer class (without whitespace as a token)
class CharacterTokenizer:
    def __init__(self):
        self.vocab = {chr(i): i - 97 for i in range(97, 123)}  # a-z
        self.vocab['[MASK]'] = len(self.vocab)
        self.vocab['[PAD]'] = len(self.vocab) + 1
        self.vocab['[UNK]'] = 0

    def tokenize(self, word):
        return [char if char != '_' else '[MASK]' for char in word]

    def convert_tokens_to_ids(self, tokens):
        return [self.vocab.get(t, self.vocab['[UNK]']) for t in tokens]

    def convert_ids_to_tokens(self, ids):
        inv_vocab = {v: k for k, v in self.vocab.items()}
        return [inv_vocab.get(i, '[UNK]') for i in ids]

# Initialize tokenizer
tokenizer = CharacterTokenizer()

# Define BERT model configuration
config = BertConfig(
    vocab_size=len(tokenizer.vocab),
    max_position_embeddings=max_word_length,
    hidden_size=hidden_size,
    num_attention_heads=num_attention_heads,
    num_hidden_layers=num_hidden_layers
)

# Initialize the BERT model with random weights
model = BertForMaskedLM(config)

# Function to pad or truncate input and generate attention masks
def pad_or_truncate(input_ids, max_len=max_len, pad_token_id=tokenizer.vocab['[PAD]']):
    attention_mask = [1] * len(input_ids)  # Initialize attention mask as 1 for each token
    if len(input_ids) > max_len:
        input_ids = input_ids[:max_len]
        attention_mask = attention_mask[:max_len]  # Truncate attention mask
    else:
        pad_length = max_len - len(input_ids)
        input_ids += [pad_token_id] * pad_length
        attention_mask += [0] * pad_length  # Pad attention mask with 0

    return input_ids, attention_mask

# Prepare training data
def create_training_data(dictionary, tokenizer, max_len=max_len):
    inputs = []
    labels = []
    attention_masks = []
    for word in dictionary:
        tokenized_input = tokenizer.tokenize(word)
        input_ids = tokenizer.convert_tokens_to_ids(tokenized_input)
        input_ids, attention_mask = pad_or_truncate(input_ids, max_len=max_len)
        label_ids = input_ids.copy()

        # Mask some characters
        for i in range(len(input_ids)):
            if input_ids[i] != tokenizer.vocab['[PAD]'] and torch.rand(1).item() < 0.15:
                input_ids[i] = tokenizer.vocab['[MASK]']

        inputs.append(input_ids)
        labels.append(label_ids)
        attention_masks.append(attention_mask)

    return torch.tensor(inputs), torch.tensor(labels), torch.tensor(attention_masks)

inputs, labels, attention_masks = create_training_data(dictionary, tokenizer)

# Create a DataLoader for batching
dataset = TensorDataset(inputs, labels, attention_masks)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Training the model
def fine_tune_model(data_loader, model, epochs=epochs, learning_rate=learning_rate):
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch in data_loader:
            input_ids, label_ids, attention_mask = batch

            optimizer.zero_grad()
            outputs = model(input_ids, labels=label_ids, attention_mask=attention_mask)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            if (batch_idx + 1) % 100 == 0:
                print(f"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}")

        print(f"Epoch {epoch+1} complete, Average Loss: {total_loss / len(data_loader)}")

fine_tune_model(data_loader, model)

# Save the trained model
def save_model_and_tokenizer(model, tokenizer, model_path=model_path, tokenizer_path=tokenizer_path):
    model.save_pretrained(model_path)

    # Save the tokenizer
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)

# Load the trained model and tokenizer
def load_model_and_tokenizer(model_path=model_path, tokenizer_path=tokenizer_path):
    from transformers import BertForMaskedLM

    model = BertForMaskedLM.from_pretrained(model_path)

    # Load the tokenizer
    with open(tokenizer_path, 'rb') as f:
        tokenizer = pickle.load(f)

    return model, tokenizer

# Example usage
save_model_and_tokenizer(model, tokenizer)

# Load model and tokenizer
model, tokenizer = load_model_and_tokenizer()

# Implement the guessing strategy
def guess_next_letter(current_word, guessed_letters, model, tokenizer, max_len=max_len):
    tokenized_input = tokenizer.tokenize(current_word)
    input_ids = tokenizer.convert_tokens_to_ids(tokenized_input)
    input_ids, attention_mask = pad_or_truncate(input_ids, max_len=max_len)
    inputs = torch.tensor([input_ids])
    attention_mask = torch.tensor([attention_mask])

    model.eval()
    with torch.no_grad():
        outputs = model(inputs, attention_mask=attention_mask)

    predictions = outputs.logits[0]

    # Find the best prediction that hasn't been guessed yet
    for idx, token in enumerate(tokenized_input):
        if token == '[MASK]':
            char_logits = predictions[idx]
            sorted_indices = torch.argsort(char_logits, descending=True)
            for pred_idx in sorted_indices:
                predicted_char = tokenizer.convert_ids_to_tokens([pred_idx.item()])[0]
                if predicted_char not in guessed_letters:
                    return predicted_char

# Example usage
current_word = '_ e _ _ e t _'
guessed_letters = {'e', 't'}
next_guess = guess_next_letter(current_word, guessed_letters, model, tokenizer)
print(f"Next guessed letter: {next_guess}")
