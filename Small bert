from transformers import BertConfig, BertForMaskedLM, get_linear_schedule_with_warmup
import torch
import torch.nn as nn
import pickle
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW

# Define variables
max_word_length = 15
hidden_size = 128  # Smaller model
num_attention_heads = 2  # Fewer attention heads
num_hidden_layers = 3  # Fewer hidden layers
model_path = 'path/to/save/model'
tokenizer_path = 'path/to/save/tokenizer.pkl'
epochs = 3
learning_rate = 1e-4
max_len = 15
batch_size = 32
weight_decay = 1e-2  # Adding weight decay
dropout_prob = 0.3  # Adding dropout
dictionary = ["apple", "banana", "orange", "grape", "melon", "supercalifragilisticexpialidocious"]

# Define the character-level tokenizer class (without whitespace as a token)
class CharacterTokenizer:
    def __init__(self):
        self.vocab = {chr(i): i - 97 for i in range(97, 123)}  # a-z
        self.vocab['[MASK]'] = len(self.vocab)
        self.vocab['[PAD]'] = len(self.vocab) + 1
        self.vocab['[UNK]'] = 0

    def tokenize(self, word):
        return [char if char != '_' else '[MASK]' for char in word]

    def convert_tokens_to_ids(self, tokens):
        return [self.vocab.get(t, self.vocab['[UNK]']) for t in tokens]

    def convert_ids_to_tokens(self, ids):
        inv_vocab = {v: k for k, v in self.vocab.items()}
        return [inv_vocab.get(i, '[UNK]') for i in ids]

# Initialize tokenizer
tokenizer = CharacterTokenizer()

# Define BERT model configuration
config = BertConfig(
    vocab_size=len(tokenizer.vocab),
    max_position_embeddings=max_word_length,
    hidden_size=hidden_size,
    num_attention_heads=num_attention_heads,
    num_hidden_layers=num_hidden_layers,
    hidden_dropout_prob=dropout_prob  # Adding dropout
)

# Initialize the BERT model with random weights
model = BertForMaskedLM(config)

# Function to pad or truncate input and generate attention masks
def pad_or_truncate(input_ids, max_len=max_len, pad_token_id=tokenizer.vocab['[PAD]']):
    attention_mask = [1] * len(input_ids)  # Initialize attention mask as 1 for each token
    if len(input_ids) > max_len:
        input_ids = input_ids[:max_len]
        attention_mask = attention_mask[:max_len]  # Truncate attention mask
    else:
        pad_length = max_len - len(input_ids)
        input_ids += [pad_token_id] * pad_length
        attention_mask += [0] * pad_length  # Pad attention mask with 0

    return input_ids, attention_mask

# Prepare training data with dynamic masking
def create_training_data(dictionary, tokenizer, max_len=max_len):
    inputs = []
    labels = []
    attention_masks = []
    for word in dictionary:
        tokenized_input = tokenizer.tokenize(word)
        input_ids = tokenizer.convert_tokens_to_ids(tokenized_input)
        input_ids, attention_mask = pad_or_truncate(input_ids, max_len=max_len)
        label_ids = input_ids.copy()

        # Dynamic masking: mask 15% of the input tokens randomly
        for i in range(len(input_ids)):
            if input_ids[i] != tokenizer.vocab['[PAD]'] and torch.rand(1).item() < 0.15:
                input_ids[i] = tokenizer.vocab['[MASK]']

        inputs.append(input_ids)
        labels.append(label_ids)
        attention_masks.append(attention_mask)

    return torch.tensor(inputs), torch.tensor(labels), torch.tensor(attention_masks)

inputs, labels, attention_masks = create_training_data(dictionary, tokenizer)

# Create a DataLoader for batching
dataset = TensorDataset(inputs, labels, attention_masks)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Label smoothing
class LabelSmoothingLoss(nn.Module):
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        log_prob = nn.functional.log_softmax(pred, dim=-1)
        true_dist = torch.zeros_like(log_prob)
        true_dist.fill_(self.smoothing / (log_prob.size(-1) - 1))
        true_dist.scatter_(1, target.unsqueeze(1), (1 - self.smoothing))
        return torch.mean(torch.sum(-true_dist * log_prob, dim=-1))

# Training the model with optimizations
def fine_tune_model(data_loader, model, epochs=epochs, learning_rate=learning_rate, weight_decay=weight_decay):
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    criterion = LabelSmoothingLoss()  # Using label smoothing
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=epochs * len(data_loader))
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch_idx, batch in enumerate(data_loader):
            input_ids, label_ids, attention_mask = batch

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            loss = criterion(logits.view(-1, logits.size(-1)), label_ids.view(-1))
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()
            if (batch_idx + 1) % 100 == 0:
                print(f"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}")

        print(f"Epoch {epoch+1} complete, Average Loss: {total_loss / len(data_loader)}")

fine_tune_model(data_loader, model)

# Save the trained model
def save_model_and_tokenizer(model, tokenizer, model_path=model_path, tokenizer_path=tokenizer_path):
    model.save_pretrained(model_path)

    # Save the tokenizer
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)

# Example usage
save_model_and_tokenizer(model, tokenizer)
